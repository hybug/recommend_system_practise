## 多臂赌博机问题(Multi-armed bandit problem)
原始问题：一个赌徒摇老虎机，走进赌场一看，一排老虎机外表一模一样，但每个老虎机吐钱
的概率不一样，它不知道老虎机吐钱概率分布，那么如何最大化收益？

在推荐系统中也存在着多臂赌博机问题：

1. 假设一个用户对不同类别内容感兴趣程度不同，当推荐系统初次见到用户时，怎么快速直到他
对每类内容的感兴趣程度？这也是推荐系统经常面对的冷启动问题。

2. 假设系统中有若干广告库存，该给每个用户展示哪个广告才能获得最大点击收益，是不是每次
都挑收益最好的那个？

3. 算法工程师又设计出了新的策略或者模型，如何既能直到它和旧模型相比谁更可靠，又能控制
风险？

There are choices, there are profit, there a MAB problem.

## 探索-利用问题（Exploitation-Exploration Problem）

探索：被不断探索用户新的兴趣，否则将和快就会出现大量重复推荐

利用：比较确定的兴趣，要重充分利用，进行实际推荐

## 置信区间上界算法(Upper Confidence Bound)

前面提到了，Epsilon-Greedy算法在探索的时候，所有的老虎机都有同样的概率被选中，这其
实没有充分利用历史信息，比如每个老虎机之前探索的次数，每个老虎机之前的探索中吐钱的频率。

那我们怎么能够充分利用历史信息呢？首先，根据当前老虎机已经探索的次数，以及吐钱的次数，我们可
以计算出当前每个老虎机吐钱的观测概率p'。同时，由于观测次数有限，因此观测概率和真实概率p之间总
会有一定的差值 ∆ ，即p' - ∆ <= p <= p' + ∆。

基于上面的讨论，我们得到了另一种常用的Bandit算法：UCB(Upper Confidence Bound)算法。该算法在每次
推荐时，总是乐观的认为每个老虎机能够得到的收益是p' + ∆。

好了，接下来的问题就是观测概率和真实概率之间的差值∆如何计算了，我们首先有两个直观的理解：
1. 对于选中的老虎机，多获得一次反馈会使∆变小，当反馈无穷多时，∆趋近于0，最终会小于其他没有被选中的老虎机的∆。
2. 对于没有被选中的老虎机，∆会随着轮数的增大而增加，最终会大于其他被选中的老虎机。

因此，当进行了一定的轮数的时候，每个老虎机都有机会得到探索的机会。UCB算法中p' + ∆的计算公式如下：

np.sqrt(2 * np.log(T) / chosen_cnt[item])
